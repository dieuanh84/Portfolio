{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUsAYKXWddkw",
        "outputId": "f6ad83b7-0039-4e1c-f82e-ad74fa586232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaB2vlhHgQ9y",
        "outputId": "2cbdc199-b285-405a-80ca-69f7bb0e197a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "return.csv\n",
            "pe.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-bb8970ba0dea>:15: DtypeWarning: Columns (7,66) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df[file[:-4]] = pd.read_csv(file_path)  # file[:-4] to remove '.csv' from the filename\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "volume.csv\n",
            "volatility.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder_path = '/content/drive/My Drive/Team2/ML project input data/'\n",
        "\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "df = {}\n",
        "\n",
        "for file in files:\n",
        "    if file.endswith(\".csv\"):\n",
        "      print(file)\n",
        "      file_path = os.path.join(folder_path, file)\n",
        "      # Read the CSV file and store it in a dictionary with the filename as the key\n",
        "      df[file[:-4]] = pd.read_csv(file_path)  # file[:-4] to remove '.csv' from the filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljc-e9zSg5dl"
      },
      "outputs": [],
      "source": [
        "df_return = df['return']\n",
        "df_pe = df['pe']\n",
        "df_volatility = df['volatility']\n",
        "df_volume = df['volume']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEoEdYliAOwI"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjlNDn5xAvQK"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, features, target, lookback):\n",
        "    X, y = [], []\n",
        "    for i in range(lookback, len(data)):\n",
        "        X.append(data[features].iloc[i-lookback:i].values)\n",
        "        y.append(data[target].iloc[i])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBQmYaYRIqXl"
      },
      "outputs": [],
      "source": [
        "features = ['volatility', 'P/E', 'volume', 'return']\n",
        "target = 'return'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq5ot8wSjQPw"
      },
      "outputs": [],
      "source": [
        "performance = {}\n",
        "outcome = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Batch 1: Company 1 - 10**"
      ],
      "metadata": {
        "id": "29xFv_mlebtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v7rx3k-C-4XW",
        "outputId": "599b73d4-84ba-41d2-a718-eb6bb1b12a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 3s 21ms/step\n",
            "19/19 [==============================] - 0s 23ms/step\n",
            "Done with VTM.JST.CMLBK.FOR INTRD.\n",
            "35/35 [==============================] - 1s 16ms/step\n",
            "19/19 [==============================] - 1s 15ms/step\n",
            "Done with VIETNAM INTERNATIONAL COMMERCIAL\n",
            "105/105 [==============================] - 2s 15ms/step\n",
            "19/19 [==============================] - 0s 22ms/step\n",
            "Done with THANH THANH CONG - BIEN HOA\n",
            "106/106 [==============================] - 2s 15ms/step\n",
            "19/19 [==============================] - 0s 24ms/step\n",
            "Done with PETROVIETNAM FCM.\n",
            "88/88 [==============================] - 2s 16ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with HA DO GROUP\n",
            "96/96 [==============================] - 3s 17ms/step\n",
            "19/19 [==============================] - 0s 15ms/step\n",
            "Done with PHUNHUAN JEWELRY\n",
            "81/81 [==============================] - 2s 16ms/step\n",
            "19/19 [==============================] - 0s 20ms/step\n",
            "Done with ELCOM TECHNOLOGY COMMUNICATIONS\n",
            "58/58 [==============================] - 2s 18ms/step\n",
            "19/19 [==============================] - 0s 15ms/step\n",
            "Done with FECON MINING\n",
            "104/104 [==============================] - 3s 17ms/step\n",
            "19/19 [==============================] - 0s 16ms/step\n",
            "Done with DONG PHU RUBBER\n",
            "68/68 [==============================] - 2s 15ms/step\n",
            "19/19 [==============================] - 0s 22ms/step\n",
            "Done with CNG VIETNAM\n"
          ]
        }
      ],
      "source": [
        "for c in [\"VTM.JST.CMLBK.FOR INTRD.\", \"VIETNAM INTERNATIONAL COMMERCIAL\", \"THANH THANH CONG - BIEN HOA\", \"PETROVIETNAM FCM.\", \"HA DO GROUP\", \"PHUNHUAN JEWELRY\", \"ELCOM TECHNOLOGY COMMUNICATIONS\", \"FECON MINING\", \"DONG PHU RUBBER\", \"CNG VIETNAM\"]:\n",
        "  # Get a specific stock's data\n",
        "  df_comp = df_return[['Date', c]]\n",
        "  df_comp.rename(columns = {c: \"return\"}, inplace=True)\n",
        "  df_comp['date'] = pd.to_datetime(df_comp['Date'])\n",
        "  # merge data to get volatility, P/E ratio, and volume\n",
        "  df_comp = pd.merge(df_comp, df_volatility[['Date', c]], on='Date')\n",
        "  df_comp = pd.merge(df_comp, df_pe[['Date', c]], on='Date')\n",
        "  df_comp = pd.merge(df_comp, df_volume[['Date', c]], on='Date')\n",
        "  df_comp.rename(columns = {c+'_x': 'volatility', c+'_y': 'P/E', c: 'volume'}, inplace=True)\n",
        "  df_comp = df_comp.drop(columns = ['Date'])\n",
        "  df_comp = df_comp.dropna()\n",
        "\n",
        "  features = ['volatility', 'P/E', 'volume', 'return']\n",
        "  target = 'return'\n",
        "\n",
        "  # Scale for each columns\n",
        "  df_comp_scaled = pd.DataFrame()\n",
        "  for f in features:\n",
        "    df_comp[f+\"_mean\"] = df_comp[f].rolling(window=60, min_periods=60).mean()\n",
        "    df_comp[f+\"_std\"] = df_comp[f].rolling(window=60, min_periods=60).std()\n",
        "    df_comp_scaled[f] = (df_comp[f] - df_comp[f+\"_mean\"])/df_comp[f+\"_std\"]\n",
        "  df_comp_scaled = df_comp_scaled.reset_index()\n",
        "  df_comp_scaled = df_comp_scaled.dropna()\n",
        "\n",
        "  lookback = 60  # Can test different number of days to look back to\n",
        "  X, y = create_sequences(df_comp_scaled, features, target, lookback)\n",
        "\n",
        "  # Train - Test split\n",
        "  train_size =  len(X) - len(df_comp[df_comp[\"date\"] >= \"2022-01-01\"])\n",
        "  X_train, X_test = X[:train_size], X[train_size:]\n",
        "  y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units=50, return_sequences=False))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units=1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "  lstm = model.fit(X_train, y_train, epochs=30, batch_size= 32, validation_split=0.2, verbose=0, shuffle=False)\n",
        "\n",
        "  trainPredict = model.predict(X_train)\n",
        "  testPredict = model.predict(X_test)\n",
        "\n",
        "  trainPredict1 = np.array([i[0] for i in trainPredict])*np.array(df_comp[\"return_std\"][-len(X):-len(testPredict)]) + df_comp[\"return_mean\"][-len(X):-len(testPredict)]\n",
        "  trainY = (y_train*df_comp[\"return_std\"][-len(X):-len(testPredict)]) + df_comp[\"return_mean\"][-len(X):-len(testPredict)]\n",
        "  testPredict1 = np.array([i[0] for i in testPredict])*np.array(df_comp[\"return_std\"][-len(testPredict):]) + df_comp[\"return_mean\"][-len(testPredict):]\n",
        "  testY = (y_test*df_comp[\"return_std\"][-len(testPredict):]) + df_comp[\"return_mean\"][-len(testPredict):]\n",
        "\n",
        "\n",
        "  trainScore = np.sqrt(mean_squared_error(trainY, trainPredict1))\n",
        "  testScore = np.sqrt(mean_squared_error(testY, testPredict1))\n",
        "\n",
        "  performance[c] = [trainScore, testScore]\n",
        "\n",
        "  l = [df_comp.date[-len(y_test):], testY, testPredict1]\n",
        "  pred_comp = pd.DataFrame()\n",
        "  pred_comp[\"date\"] = l[0]\n",
        "  pred_comp[\"actual_return\"] = l[1]\n",
        "  pred_comp[\"predicted_return\"] = l[2]\n",
        "  outcome.append(pred_comp)\n",
        "\n",
        "  print(\"Done with \"+c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6iwIVs5igug"
      },
      "outputs": [],
      "source": [
        "for i, c in enumerate([\"VTM.JST.CMLBK.FOR INTRD.\", \"VIETNAM INTERNATIONAL COMMERCIAL\", \"THANH THANH CONG - BIEN HOA\", \"PETROVIETNAM FCM.\",\n",
        "          \"HA DO GROUP\", \"PHUNHUAN JEWELRY\", \"ELCOM TECHNOLOGY COMMUNICATIONS\", \"FECON MINING\", \"DONG PHU RUBBER\", \"CNG VIETNAM\"]):\n",
        "  outcome[i].to_csv(f\"/content/drive/MyDrive/Team2/ML project output data/{c}.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjrcn5YflGp4"
      },
      "source": [
        "**Training Batch 2: Company 11 - 20**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-WFGdrClOTi",
        "outputId": "e44aab0e-05f9-4405-e2b4-8fb83be2fbc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37/37 [==============================] - 1s 16ms/step\n",
            "19/19 [==============================] - 0s 16ms/step\n",
            "Done with GELEX GROUP\n",
            "50/50 [==============================] - 2s 17ms/step\n",
            "19/19 [==============================] - 1s 17ms/step\n",
            "Done with MOBILE WORLD INVESTMENT\n",
            "85/85 [==============================] - 2s 17ms/step\n",
            "19/19 [==============================] - 0s 18ms/step\n",
            "Done with AN PHAT PLAST.&GRN.ENVM.\n",
            "113/113 [==============================] - 3s 17ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with VIETNAM DAIRY PRODUCTS\n",
            "45/45 [==============================] - 1s 17ms/step\n",
            "19/19 [==============================] - 0s 16ms/step\n",
            "Done with DUCGIANG CHEMICALS GROUP\n",
            "111/111 [==============================] - 3s 21ms/step\n",
            "19/19 [==============================] - 0s 21ms/step\n",
            "Done with DANANG RUBBER\n",
            "22/22 [==============================] - 1s 18ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with FIRST REAL JOINT STOCK COMPANY\n",
            "111/111 [==============================] - 3s 17ms/step\n",
            "19/19 [==============================] - 0s 18ms/step\n",
            "Done with VINH SON-SONG HINH HYPW.\n",
            "111/111 [==============================] - 3s 17ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with PETROLIMEX GAS\n",
            "88/88 [==============================] - 2s 16ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with INDUSTRIAL URBAN DEV.\n"
          ]
        }
      ],
      "source": [
        "for c in [\"GELEX GROUP\", \"MOBILE WORLD INVESTMENT\", \"AN PHAT PLAST.&GRN.ENVM.\", \"VIETNAM DAIRY PRODUCTS\", \"DUCGIANG CHEMICALS GROUP\",\n",
        "                       \"DANANG RUBBER\",\"FIRST REAL JOINT STOCK COMPANY\",\"VINH SON-SONG HINH HYPW.\",\"PETROLIMEX GAS\", \"INDUSTRIAL URBAN DEV.\"]:\n",
        "  # Get a specific stock's data\n",
        "  df_comp = df_return[['Date', c]]\n",
        "  df_comp.rename(columns = {c: \"return\"}, inplace=True)\n",
        "  df_comp['date'] = pd.to_datetime(df_comp['Date'])\n",
        "  # merge data to get volatility, P/E ratio, and volume\n",
        "  df_comp = pd.merge(df_comp, df_volatility[['Date', c]], on='Date')\n",
        "  df_comp = pd.merge(df_comp, df_pe[['Date', c]], on='Date')\n",
        "  df_comp = pd.merge(df_comp, df_volume[['Date', c]], on='Date')\n",
        "  df_comp.rename(columns = {c+'_x': 'volatility', c+'_y': 'P/E', c: 'volume'}, inplace=True)\n",
        "  df_comp = df_comp.drop(columns = ['Date'])\n",
        "  df_comp = df_comp.dropna()\n",
        "\n",
        "  features = ['volatility', 'P/E', 'volume', 'return']\n",
        "  target = 'return'\n",
        "\n",
        "  # Scale for each columns\n",
        "  df_comp_scaled = pd.DataFrame()\n",
        "  for f in features:\n",
        "    df_comp[f+\"_mean\"] = df_comp[f].rolling(window=60, min_periods=60).mean()\n",
        "    df_comp[f+\"_std\"] = df_comp[f].rolling(window=60, min_periods=60).std()\n",
        "    df_comp_scaled[f] = (df_comp[f] - df_comp[f+\"_mean\"])/df_comp[f+\"_std\"]\n",
        "  df_comp_scaled = df_comp_scaled.reset_index()\n",
        "  df_comp_scaled = df_comp_scaled.dropna()\n",
        "\n",
        "  lookback = 60  # Can test different number of days to look back to\n",
        "  X, y = create_sequences(df_comp_scaled, features, target, lookback)\n",
        "\n",
        "  # Train - Test split\n",
        "  train_size =  len(X) - len(df_comp[df_comp[\"date\"] >= \"2022-01-01\"])\n",
        "  X_train, X_test = X[:train_size], X[train_size:]\n",
        "  y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units=50, return_sequences=False))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units=1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "  lstm = model.fit(X_train, y_train, epochs=30, batch_size= 32, validation_split=0.2, verbose=0, shuffle=False)\n",
        "\n",
        "  trainPredict = model.predict(X_train)\n",
        "  testPredict = model.predict(X_test)\n",
        "\n",
        "  trainPredict1 = np.array([i[0] for i in trainPredict])*np.array(df_comp[\"return_std\"][-len(X):-len(testPredict)]) + df_comp[\"return_mean\"][-len(X):-len(testPredict)]\n",
        "  trainY = (y_train*df_comp[\"return_std\"][-len(X):-len(testPredict)]) + df_comp[\"return_mean\"][-len(X):-len(testPredict)]\n",
        "  testPredict1 = np.array([i[0] for i in testPredict])*np.array(df_comp[\"return_std\"][-len(testPredict):]) + df_comp[\"return_mean\"][-len(testPredict):]\n",
        "  testY = (y_test*df_comp[\"return_std\"][-len(testPredict):]) + df_comp[\"return_mean\"][-len(testPredict):]\n",
        "\n",
        "\n",
        "  trainScore = np.sqrt(mean_squared_error(trainY, trainPredict1))\n",
        "  testScore = np.sqrt(mean_squared_error(testY, testPredict1))\n",
        "\n",
        "  performance[c] = [trainScore, testScore]\n",
        "\n",
        "  l = [df_comp.date[-len(y_test):], testY, testPredict1]\n",
        "  pred_comp = pd.DataFrame()\n",
        "  pred_comp[\"date\"] = l[0]\n",
        "  pred_comp[\"actual_return\"] = l[1]\n",
        "  pred_comp[\"predicted_return\"] = l[2]\n",
        "  outcome.append(pred_comp)\n",
        "\n",
        "  print(\"Done with \"+c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rvS3xH3hlRwi"
      },
      "outputs": [],
      "source": [
        "for i, c in enumerate([\"GELEX GROUP\", \"MOBILE WORLD INVESTMENT\", \"AN PHAT PLAST.&GRN.ENVM.\", \"VIETNAM DAIRY PRODUCTS\", \"DUCGIANG CHEMICALS GROUP\",\n",
        "                       \"DANANG RUBBER\",\"FIRST REAL JOINT STOCK COMPANY\",\"VINH SON-SONG HINH HYPW.\",\"PETROLIMEX GAS\", \"INDUSTRIAL URBAN DEV.\"]):\n",
        "  outcome[i].to_csv(f\"/content/drive/MyDrive/Team2/ML project output data/{c}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzlWQzKersVG"
      },
      "source": [
        "**Training Batch 3: Company 21 - 30**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOpE9xWNr1lN",
        "outputId": "4c3e4a82-88c4-48e5-f25e-222557086171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 4s 25ms/step\n",
            "19/19 [==============================] - 1s 17ms/step\n",
            "Done with SSI SECURITIES\n",
            "81/81 [==============================] - 2s 17ms/step\n",
            "19/19 [==============================] - 0s 16ms/step\n",
            "Done with PHAT DAT RLST.DEV.\n",
            "111/111 [==============================] - 3s 17ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with FPT\n",
            "94/94 [==============================] - 2s 17ms/step\n",
            "19/19 [==============================] - 0s 25ms/step\n",
            "Done with JST.CMLBK.FOR FRT. OF VIET NAM\n",
            "61/61 [==============================] - 2s 24ms/step\n",
            "19/19 [==============================] - 1s 28ms/step\n",
            "Done with NAM LONG INVESTMENT\n",
            "102/102 [==============================] - 2s 17ms/step\n",
            "19/19 [==============================] - 0s 16ms/step\n",
            "Done with GEMADEPT\n",
            "89/89 [==============================] - 2s 17ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with BAOVIET HOLDINGS\n",
            "108/108 [==============================] - 3s 21ms/step\n",
            "19/19 [==============================] - 1s 25ms/step\n",
            "Done with VIET NAM TANKER\n",
            "77/77 [==============================] - 3s 24ms/step\n",
            "19/19 [==============================] - 1s 26ms/step\n",
            "Done with PHU TAI\n",
            "109/109 [==============================] - 3s 16ms/step\n",
            "19/19 [==============================] - 0s 16ms/step\n",
            "Done with SAO TA FOODS\n"
          ]
        }
      ],
      "source": [
        "for c in [\"SSI SECURITIES\", \"PHAT DAT RLST.DEV.\", \"FPT\", \"JST.CMLBK.FOR FRT. OF VIET NAM\", \"NAM LONG INVESTMENT\",\n",
        "                       \"GEMADEPT\", \"BAOVIET HOLDINGS\", \"VIET NAM TANKER\", \"PHU TAI\", \"SAO TA FOODS\"]:\n",
        "  # Get a specific stock's data\n",
        "  df_comp = df_return[['Date', c]]\n",
        "  df_comp.rename(columns = {c: \"return\"}, inplace=True)\n",
        "  df_comp['date'] = pd.to_datetime(df_comp['Date'])\n",
        "  # merge data to get volatility, P/E ratio, and volume\n",
        "  df_comp = pd.merge(df_comp, df_volatility[['Date', c]], on='Date')\n",
        "  df_comp = pd.merge(df_comp, df_pe[['Date', c]], on='Date')\n",
        "  df_comp = pd.merge(df_comp, df_volume[['Date', c]], on='Date')\n",
        "  df_comp.rename(columns = {c+'_x': 'volatility', c+'_y': 'P/E', c: 'volume'}, inplace=True)\n",
        "  df_comp = df_comp.drop(columns = ['Date'])\n",
        "  df_comp = df_comp.dropna()\n",
        "\n",
        "  features = ['volatility', 'P/E', 'volume', 'return']\n",
        "  target = 'return'\n",
        "\n",
        "  # Scale for each columns\n",
        "  df_comp_scaled = pd.DataFrame()\n",
        "  for f in features:\n",
        "    df_comp[f+\"_mean\"] = df_comp[f].rolling(window=60, min_periods=60).mean()\n",
        "    df_comp[f+\"_std\"] = df_comp[f].rolling(window=60, min_periods=60).std()\n",
        "    df_comp_scaled[f] = (df_comp[f] - df_comp[f+\"_mean\"])/df_comp[f+\"_std\"]\n",
        "  df_comp_scaled = df_comp_scaled.reset_index()\n",
        "  df_comp_scaled = df_comp_scaled.dropna()\n",
        "\n",
        "  lookback = 60  # Can test different number of days to look back to\n",
        "  X, y = create_sequences(df_comp_scaled, features, target, lookback)\n",
        "\n",
        "  # Train - Test split\n",
        "  train_size =  len(X) - len(df_comp[df_comp[\"date\"] >= \"2022-01-01\"])\n",
        "  X_train, X_test = X[:train_size], X[train_size:]\n",
        "  y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units=50, return_sequences=False))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units=1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "  lstm = model.fit(X_train, y_train, epochs=30, batch_size= 32, validation_split=0.2, verbose=0, shuffle=False)\n",
        "\n",
        "  trainPredict = model.predict(X_train)\n",
        "  testPredict = model.predict(X_test)\n",
        "\n",
        "  trainPredict1 = np.array([i[0] for i in trainPredict])*np.array(df_comp[\"return_std\"][-len(X):-len(testPredict)]) + df_comp[\"return_mean\"][-len(X):-len(testPredict)]\n",
        "  trainY = (y_train*df_comp[\"return_std\"][-len(X):-len(testPredict)]) + df_comp[\"return_mean\"][-len(X):-len(testPredict)]\n",
        "  testPredict1 = np.array([i[0] for i in testPredict])*np.array(df_comp[\"return_std\"][-len(testPredict):]) + df_comp[\"return_mean\"][-len(testPredict):]\n",
        "  testY = (y_test*df_comp[\"return_std\"][-len(testPredict):]) + df_comp[\"return_mean\"][-len(testPredict):]\n",
        "\n",
        "\n",
        "  trainScore = np.sqrt(mean_squared_error(trainY, trainPredict1))\n",
        "  testScore = np.sqrt(mean_squared_error(testY, testPredict1))\n",
        "\n",
        "  performance[c] = [trainScore, testScore]\n",
        "\n",
        "  l = [df_comp.date[-len(y_test):], testY, testPredict1]\n",
        "  pred_comp = pd.DataFrame()\n",
        "  pred_comp[\"date\"] = l[0]\n",
        "  pred_comp[\"actual_return\"] = l[1]\n",
        "  pred_comp[\"predicted_return\"] = l[2]\n",
        "  outcome.append(pred_comp)\n",
        "\n",
        "  print(\"Done with \"+c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDjmtellr4FH"
      },
      "outputs": [],
      "source": [
        "for i, c in enumerate([\"SSI SECURITIES\", \"PHAT DAT RLST.DEV.\", \"FPT\", \"JST.CMLBK.FOR FRT. OF VIET NAM\", \"NAM LONG INVESTMENT\",\n",
        "                       \"GEMADEPT\", \"BAOVIET HOLDINGS\", \"VIET NAM TANKER\", \"PHU TAI\", \"SAO TA FOODS\"]):\n",
        "  outcome[i].to_csv(f\"/content/drive/MyDrive/Team2/ML project output data/{c}.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3BILRF6PgvN"
      },
      "source": [
        "**Training Batch 4: Company 31 - 40**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8xtuquGPcGm",
        "outputId": "2650a4ed-62f1-4503-a805-0d8c625422b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "74/74 [==============================] - 2s 19ms/step\n",
            "19/19 [==============================] - 1s 27ms/step\n",
            "Done with MILITARY COML.JST.BANK\n",
            "54/54 [==============================] - 2s 17ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with BANK FOR INVDL.OF VTM.\n",
            "95/95 [==============================] - 2s 16ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with HOCHIMINH CTY.SECS.\n",
            "86/86 [==============================] - 2s 17ms/step\n",
            "19/19 [==============================] - 0s 17ms/step\n",
            "Done with BECAMEX INFR.DEVELOPMENT\n",
            "45/45 [==============================] - 2s 21ms/step\n",
            "19/19 [==============================] - 0s 23ms/step\n",
            "Done with DIGIWORLD\n",
            "103/103 [==============================] - 3s 17ms/step\n",
            "19/19 [==============================] - 0s 18ms/step\n",
            "Done with BA RIA-VUNG TAU HSE.DEV.\n",
            "101/101 [==============================] - 3s 21ms/step\n",
            "19/19 [==============================] - 0s 20ms/step\n",
            "Done with VINH HOAN\n",
            "90/90 [==============================] - 2s 18ms/step\n",
            "12/12 [==============================] - 0s 17ms/step\n",
            "Done with CMC\n",
            "24/24 [==============================] - 2s 19ms/step\n",
            "19/19 [==============================] - 0s 18ms/step\n",
            "Done with HONG HA FOOD INVESTMENT DEVELOPMENT\n",
            "81/81 [==============================] - 2s 18ms/step\n",
            "19/19 [==============================] - 0s 18ms/step\n",
            "Done with BINH MINH PLASTICS\n"
          ]
        }
      ],
      "source": [
        "for c in [\"MILITARY COML.JST.BANK\", \"BANK FOR INVDL.OF VTM.\", \"HOCHIMINH CTY.SECS.\", \"BECAMEX INFR.DEVELOPMENT\",\"DIGIWORLD\",\n",
        "          \"BA RIA-VUNG TAU HSE.DEV.\", \"VINH HOAN\", \"CMC\", \"HONG HA FOOD INVESTMENT DEVELOPMENT\", \"BINH MINH PLASTICS\"]:\n",
        "  # Get a specific stock's data\n",
        "  df_comp = df_return[['Date', c]]\n",
        "  df_comp.rename(columns = {c: \"return\"}, inplace=True)\n",
        "  df_comp['date'] = pd.to_datetime(df_comp['Date'])\n",
        "  # merge data to get volatility, P/E ratio, and volume\n",
        "  df_comp = pd.merge(df_comp, df_volatility[['Date', c]], on='Date')\n",
        "  df_comp = pd.merge(df_comp, df_pe[['Date', c]], on='Date')\n",
        "  df_comp = pd.merge(df_comp, df_volume[['Date', c]], on='Date')\n",
        "  df_comp.rename(columns = {c+'_x': 'volatility', c+'_y': 'P/E', c: 'volume'}, inplace=True)\n",
        "  df_comp = df_comp.drop(columns = ['Date'])\n",
        "  df_comp = df_comp.dropna()\n",
        "\n",
        "  features = ['volatility', 'P/E', 'volume', 'return']\n",
        "  target = 'return'\n",
        "\n",
        "  # Scale for each columns\n",
        "  df_comp_scaled = pd.DataFrame()\n",
        "  for f in features:\n",
        "    df_comp[f+\"_mean\"] = df_comp[f].rolling(window=60, min_periods=60).mean()\n",
        "    df_comp[f+\"_std\"] = df_comp[f].rolling(window=60, min_periods=60).std()\n",
        "    df_comp_scaled[f] = (df_comp[f] - df_comp[f+\"_mean\"])/df_comp[f+\"_std\"]\n",
        "  df_comp_scaled = df_comp_scaled.reset_index()\n",
        "  df_comp_scaled = df_comp_scaled.dropna()\n",
        "\n",
        "  lookback = 60  # Can test different number of days to look back to\n",
        "  X, y = create_sequences(df_comp_scaled, features, target, lookback)\n",
        "\n",
        "  # Train - Test split\n",
        "  train_size =  len(X) - len(df_comp[df_comp[\"date\"] >= \"2022-01-01\"])\n",
        "  X_train, X_test = X[:train_size], X[train_size:]\n",
        "  y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LSTM(units=50, return_sequences=False))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units=1))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "  lstm = model.fit(X_train, y_train, epochs=30, batch_size= 32, validation_split=0.2, verbose=0, shuffle=False)\n",
        "\n",
        "  trainPredict = model.predict(X_train)\n",
        "  testPredict = model.predict(X_test)\n",
        "\n",
        "  trainPredict1 = np.array([i[0] for i in trainPredict])*np.array(df_comp[\"return_std\"][-len(X):-len(testPredict)]) + df_comp[\"return_mean\"][-len(X):-len(testPredict)]\n",
        "  trainY = (y_train*df_comp[\"return_std\"][-len(X):-len(testPredict)]) + df_comp[\"return_mean\"][-len(X):-len(testPredict)]\n",
        "  testPredict1 = np.array([i[0] for i in testPredict])*np.array(df_comp[\"return_std\"][-len(testPredict):]) + df_comp[\"return_mean\"][-len(testPredict):]\n",
        "  testY = (y_test*df_comp[\"return_std\"][-len(testPredict):]) + df_comp[\"return_mean\"][-len(testPredict):]\n",
        "\n",
        "\n",
        "  trainScore = np.sqrt(mean_squared_error(trainY, trainPredict1))\n",
        "  testScore = np.sqrt(mean_squared_error(testY, testPredict1))\n",
        "\n",
        "  performance[c] = [trainScore, testScore]\n",
        "\n",
        "  l = [df_comp.date[-len(y_test):], testY, testPredict1]\n",
        "  pred_comp = pd.DataFrame()\n",
        "  pred_comp[\"date\"] = l[0]\n",
        "  pred_comp[\"actual_return\"] = l[1]\n",
        "  pred_comp[\"predicted_return\"] = l[2]\n",
        "  outcome.append(pred_comp)\n",
        "\n",
        "  print(\"Done with \"+c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYnysvlkP_s6"
      },
      "outputs": [],
      "source": [
        "for i, c in enumerate([\"MILITARY COML.JST.BANK\", \"BANK FOR INVDL.OF VTM.\", \"HOCHIMINH CTY.SECS.\", \"BECAMEX INFR.DEVELOPMENT\",\"DIGIWORLD\",\n",
        "          \"BA RIA-VUNG TAU HSE.DEV.\", \"VINH HOAN\", \"CMC\", \"HONG HA FOOD INVESTMENT DEVELOPMENT\", \"BINH MINH PLASTICS\"]):\n",
        "  outcome[i].to_csv(f\"/content/drive/MyDrive/Team2/ML project output data/{c}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export the train and test MSE**"
      ],
      "metadata": {
        "id": "2jqn02dCexeV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeI4oZJzreEU"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(performance).to_csv(f\"/content/drive/MyDrive/Team2/ML project output data/performance/performance.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export the trading dates of the testing period for future reference**"
      ],
      "metadata": {
        "id": "mVuQoLGjfF6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDSiAQhK3dL3"
      },
      "outputs": [],
      "source": [
        "df_comp['date'][-593:].to_csv(f\"/content/drive/MyDrive/Team2/ML project output data/performance/date_from_eikon.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}